{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HK PROTESTS: Visualising Chinese State Troll Tweets, Part 2 (Using Scattertext)\n",
    "\n",
    "Visualising unstructured text is hard. While frequency token charts and tree maps are useful in providing some quick insights into a messy body of text, they aren't particularly exciting from a visual standpoint nor useful when you want to inspect how certain key words were used in the original text.\n",
    "\n",
    "Enter [Scattertext](https://github.com/JasonKessler/scattertext), which describes itself as a \"sexy, interactive\" tool for \"distinguishing terms in small-to-medium-sized corpora\". It has some quirks, such as the hard requirement for a binary category for the text you want to analyse. But otherwise it works great out of the box, including for Chinese text (in Part 4), and the detailed tutorials will enable you to try out a wide range of visualisations.\n",
    "\n",
    "The interactive features are particularly useful in this case, allowing me to search for a key word in the chart and see which are the tweets and retweets that used that particular word or phrase. I've uploaded the interactive files to [Dropbox](https://www.dropbox.com/sh/jmb1oy0kak18cwy/AABfHXYoA_P8d6Tw-scNpDVia?dl=0) so you don't have to run this notebook in order to try out the Scattertext charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scattertext as st\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "from scipy.stats import rankdata, hmean, norm\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 300\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PRE-WORK\n",
    "The CSV files are still pretty huge after the initial rounds of pre-processing in Part 1, and will likely bust the file size limits on Github. To keep things simple, I opted to repeat the pre-processing steps so that this notebook can be run as a standalone for those who wish to skip Part 1. As long as you've downloaded the two original CSV files from Twitter, the data processing steps below should work fine.\n",
    "\n",
    "## 1.1 DATA PROCESSING\n",
    "Download the original CSV files from [Twitter](https://blog.twitter.com/en_us/topics/company/2019/information_operations_directed_at_Hong_Kong.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (15,19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Reminder that the raw1/2 CSV files are NOT in this repo. Download directly from Twitter; link above\n",
    "raw1 = pd.read_csv('../data/china_082019_1_tweets_csv_hashed.csv')\n",
    "raw2 = pd.read_csv('../data/china_082019_2_tweets_csv_hashed.csv')\n",
    "raw = pd.concat([raw1, raw2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "raw = raw.drop(\n",
    "    columns=[\n",
    "        \"user_profile_url\",\n",
    "        \"tweet_client_name\",\n",
    "        \"in_reply_to_tweetid\",\n",
    "        \"in_reply_to_userid\",\n",
    "        \"quoted_tweet_tweetid\",\n",
    "        \"is_retweet\",\n",
    "        \"retweet_userid\",\n",
    "        \"retweet_tweetid\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"quote_count\",\n",
    "        \"reply_count\",\n",
    "        \"like_count\",\n",
    "        \"retweet_count\",\n",
    "        \"urls\",\n",
    "        \"user_mentions\",\n",
    "        \"poll_choices\",\n",
    "        \"hashtags\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting timings to HK time, and extracting year-month-day-hour cols\n",
    "raw['tweet_time'] = pd.to_datetime(raw['tweet_time'])\n",
    "raw['tweet_time'] = raw['tweet_time'].dt.tz_localize('GMT').dt.tz_convert('Hongkong')\n",
    "raw['tweet_year'] = raw['tweet_time'].dt.year\n",
    "raw['tweet_month'] = raw['tweet_time'].dt.month\n",
    "raw['tweet_day'] = raw['tweet_time'].dt.day\n",
    "raw['tweet_hour'] = raw['tweet_time'].dt.hour\n",
    "\n",
    "raw['account_creation_date'] = pd.to_datetime(raw['account_creation_date'], yearfirst=True)\n",
    "raw['year_of_account_creation'] = raw['account_creation_date'].dt.year\n",
    "raw['month_of_account_creation'] = raw['account_creation_date'].dt.month\n",
    "raw['day_of_account_creation'] = raw['account_creation_date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll focus only on tweets sent from 2017\n",
    "raw = raw[(raw[\"tweet_year\"] >= 2017)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, I'll focus only on English tweets. Chinese tweets will be dealt with separately\n",
    "# In earlier drafts, I found that troll accounts with Chinese language settings were sending out English tweets too,\n",
    "# so provisions were made here to include those \n",
    "# Note the sub-categories for Twitter language settings for English and Chinese\n",
    "raw_eng = raw[\n",
    "    (raw[\"tweet_language\"] == \"en\")\n",
    "    & (\n",
    "        (raw[\"account_language\"] == \"en\")\n",
    "        | (raw[\"account_language\"] == \"en-gb\")\n",
    "        | (raw[\"account_language\"] == \"zh-cn\")\n",
    "        | (raw[\"account_language\"] == \"zh-CN\")\n",
    "        | (raw[\"account_language\"] == \"zh-tw\")\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to clean the tweet_text col\n",
    "def clean_tweet(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"W\", \" \", text)\n",
    "    text = text.strip(\" \")\n",
    "    text = text.strip(r\"\\n\")\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_eng['clean_tweet_text'] = raw_eng['tweet_text'].map(lambda tweet: clean_tweet(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DATA FILTERING\n",
    "As we've seen in Part 1, the dataset is incredibly noisy. I experimented with lightly filtered versions of the dataset and the results were highly unsatisfactory, with significant key words targetting the HK protests buried by the noise from irrelevant tweets.\n",
    "\n",
    "So I focused on a set of key words highlighted in Part 1, and further filtered for irrelevant terms which I caught in my earlier drafts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and concating a new subset of tweets with keywords of interest\n",
    "\n",
    "hk_eng1 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"hong kong\")].copy()\n",
    "hk_eng2 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"hk\")].copy()\n",
    "hk_eng3 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"police\")].copy()\n",
    "hk_eng4 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"protest\")].copy()\n",
    "hk_eng5 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"china\")].copy()\n",
    "hk_eng6 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"interference\")].copy()\n",
    "hk_eng7 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"meddling\")].copy()\n",
    "hk_eng8 = raw_eng[raw_eng['clean_tweet_text'].str.contains(\"ulterior motives\")].copy()\n",
    "\n",
    "hk_eng = pd.concat([hk_eng1, hk_eng2, hk_eng3, hk_eng4, hk_eng5, hk_eng6, hk_eng7, hk_eng8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These words popped up with high frequency in many of my initial Scattertext drafts\n",
    "# You can add or shorten this list depending on your preference\n",
    "\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"guo wengui\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"wengui\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"guo\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"poll\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"disneyland\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"world cup\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"worldcup\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"fcbayernen\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"rogerfederer\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"wimbledon\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"pew\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"mailonline\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"football\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"following\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"onthisday\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"bhivechat\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"roundtrip\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"bucket\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"jessica\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"ormsby\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"glass bridge\")].copy()\n",
    "hk_eng = hk_eng[~hk_eng[\"clean_tweet_text\"].str.contains(\"european tour\")].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SCATTERTEXT PLOTS\n",
    "\n",
    "# 2.1 Visualising Filtered English Tweets \n",
    "The highly filtered set consists of 1,108 \"original\" tweets and 1,188 retweets - a prety balanced set - and about 34,000 words. Let's see how they appear on a Scattertext chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2296, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column to classify a tweet as a retweet or actual tweet\n",
    "# This step is mandatory for the Scattertext plots\n",
    "hk_eng['tweet_status'] = np.where(hk_eng[\"tweet_text\"].str.startswith(\"RT @\"), \"retweet\", \"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several options for spacy's pre-trained English models\n",
    "# Go here if you wish to use a different one: https://spacy.io/models/en\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\") # to install, run: python -m spacy download en_core_web_lg\n",
    "\n",
    "corpus = (st.CorpusFromPandas(\n",
    "    hk_eng, \n",
    "    category_col=\"tweet_status\", \n",
    "    text_col=\"clean_tweet_text\", \n",
    "    nlp=nlp\n",
    ").build().remove_terms(ENGLISH_STOP_WORDS, ignore_absences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms most associated with tweets: ['claim', 'hate crime', 'sterling police', 'sterling', 'hate', 'osullivan', 'police investigate', 'joshua', 'basketball', 'inside the']\n",
      "Terms most associated with retweets: ['xhnews', 'cgtnofficial', 'echinanews', 'pdchina china', 'sw', 'xinhuatravel', 'xhnews china', 'party', 'pdchina chinas', 'cctv']\n"
     ]
    }
   ],
   "source": [
    "# This gives us the terms most associated with tweets and retweets in this subset:\n",
    "\n",
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df[\"Tweet Score\"] = corpus.get_scaled_f_scores(\"tweet\")\n",
    "print(\n",
    "    \"Terms most associated with tweets:\",\n",
    "    list(term_freq_df.sort_values(by=\"Tweet Score\", ascending=False).index[:10]),\n",
    ")\n",
    "\n",
    "term_freq_df[\"Retweet Score\"] = corpus.get_scaled_f_scores(\"retweet\")\n",
    "print(\n",
    "    \"Terms most associated with retweets:\",\n",
    "    list(term_freq_df.sort_values(by=\"Retweet Score\", ascending=False).index[:10]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"700\"\n",
       "            src=\"../output/scatter_eng.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a4c03bb38>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a Scattertext plot. Check out the author's repo for other options\n",
    "# Change the minimum_term_frequency if you wish to filter more aggressively\n",
    "\n",
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category=\"tweet\",\n",
    "    category_name=\"Tweets\",\n",
    "    not_category_name=\"Retweets\",\n",
    "    width_in_pixels=1000,\n",
    "    metadata=hk_eng[\"user_screen_name\"],\n",
    "    minimum_term_frequency=5,\n",
    "    show_characteristic=False,\n",
    ")\n",
    "\n",
    "interactive = \"../output/scatter_eng.html\"\n",
    "open(interactive, \"wb\").write(html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=interactive, width=1200, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the chart [here](https://www.dropbox.com/sh/jmb1oy0kak18cwy/AABfHXYoA_P8d6Tw-scNpDVia?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO INTERPRET A SCATTERTEXT CHART:\n",
    "\n",
    "### Colour\n",
    " - The words in the chart are colored by their association. Those in blue are more associated with original tweets, while those in red are more associated with retweets. Each dot corresponds to a word or phrase mentioned.\n",
    "\n",
    "### Positioning: \n",
    "- Words nearer the top of the plot represent the most frequently used words in the \"original\" tweets.\n",
    "\n",
    "- The further right a dot, the more that word or phrase was used in retweets (eg: legislative council).\n",
    "\n",
    "- Words that appear frequently in both tweets and rewteets, like \"police\" and \"china\", appear in the upper-right-hand corner.\n",
    "\n",
    "- Words that aren't often used in either tweets or retweets appear in the bottom-left-hand corner.\n",
    "\n",
    "\n",
    "### Key Areas:\n",
    "- Upper-left corner: These words appear frequently in the tweets but not the retweets. We still see a substantial amount of noise here, as evidenced by the appearance of words like \"sterling\" and \"osullivan\" near the very top.\n",
    "\n",
    "- Lower-right corner: Likewise, words which appear frequently in retweets but not the tweets appear in the lower right corner. Here we see terms which indicate which are the popular accounts from which the trolls retweet - those owned by Chinese state media outlets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH FOR KEY TERMS:\n",
    "\n",
    "One of Scattertext's best features is the search box at the bottom of each chart. Just enter a key word, such as \"police\", and see how the word was used in the tweets and retweets by the various users in the subset. This provides great context in the usage of particular key words, beyond mere frequency of appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 SCATTERTEXT PLOT FOR TOP TROLLS\n",
    "While visually appealing, the first Scattertext plot above is still very \"noisy\". Let's see if things improve with a plot of just the tweets and retweets by the two accounts most active in English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same 2 top troll accounts analysed in Part 1\n",
    "\n",
    "trolls_english = ['ctcc507', 'HKpoliticalnew']\n",
    "top_trolls = hk_eng[hk_eng['user_screen_name'].isin(trolls_english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_trolls['tweet_status'] = np.where(top_trolls[\"tweet_text\"].str.startswith(\"RT @\"), \"retweet\", \"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "corpus_trolls = (st.CorpusFromPandas(\n",
    "    top_trolls, \n",
    "    category_col=\"tweet_status\", \n",
    "    text_col=\"clean_tweet_text\", \n",
    "    nlp=nlp\n",
    ").build().remove_terms(ENGLISH_STOP_WORDS, ignore_absences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms most associated with top trolls' tweets: ['affairs', 'in kong', 'kong affairs', 'interference', 'stop', 'british', 'the police', 'china', 'southwest', 'to stop']\n"
     ]
    }
   ],
   "source": [
    "troll_term_freq_df = corpus_trolls.get_term_freq_df()\n",
    "\n",
    "troll_term_freq_df['Tweet Score'] = corpus_trolls.get_scaled_f_scores('tweet')\n",
    "print(\"Terms most associated with top trolls' tweets:\", list(troll_term_freq_df.sort_values(by='Tweet Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms most associated with top trolls' retweets: ['_', '_ _', 'protesters', 'public event', 'event', 'a public', 'hkpoliceforce', 'extraditionbill', 'chief', 'hongkong']\n"
     ]
    }
   ],
   "source": [
    "troll_term_freq_df['Retweet Score'] = corpus_trolls.get_scaled_f_scores('retweet')\n",
    "print(\"Terms most associated with top trolls' retweets:\", list(troll_term_freq_df.sort_values(by='Retweet Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"700\"\n",
       "            src=\"../output/trolls_eng.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ab3034fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "troll_html = st.produce_scattertext_explorer(\n",
    "    corpus_trolls,\n",
    "    category=\"tweet\",\n",
    "    category_name=\"Tweets\",\n",
    "    not_category_name=\"Retweets\",\n",
    "    width_in_pixels=1000,\n",
    "    metadata=top_trolls[\"user_screen_name\"],\n",
    "    show_characteristic=False,\n",
    "    minimum_term_frequency=5\n",
    ")\n",
    "\n",
    "troll_interactive = \"../output/trolls_eng.html\"\n",
    "open(troll_interactive, \"wb\").write(troll_html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=troll_interactive, width=1200, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the chart [here](https://www.dropbox.com/sh/jmb1oy0kak18cwy/AABfHXYoA_P8d6Tw-scNpDVia?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "The chart is very sparse, unsurprising given the smaller number of tweets and retweets. But we do get a good sense of what goes into the troll tweets, based on the terms that appear in the top-left corner: \"interference\", \"foreign\", \"meddling\", and \"internal\".\n",
    "\n",
    "Overall, the analysis of the English-language troll tweets has been a frustrating experience, given the very high noise-to-signal ratio. Let's see if the analysis of the Chinese-language tweets in Parts 3 and 4 are any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
