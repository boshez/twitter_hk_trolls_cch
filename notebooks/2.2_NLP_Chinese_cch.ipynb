{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HK PROTESTS: Visualising Chinese State Troll Tweets, Part 3 (Chinese Text)\n",
    "\n",
    "In Parts 3 and 4, I'll re-trace the steps in the previous two sections but for the Chinese tweets this time.\n",
    "\n",
    "I had problems plotting the frequency token distribution and tree map charts for Chinese characters, and decided to use Google Sheets instead to plot the key terms to speed up completion of this section. I'll update this notebook if I fix the font issue at some point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, jieba.analyse\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 300\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \"DE-LAYERING\" THE CHINESE TROLL TWEETS\n",
    "\n",
    "Steps below are broadly similar to Part 1, with some modifications for Chinese text, which has no word spacing, unlike English. I used the [jieba](https://github.com/fxsjy/jieba) word segmentation module for this projection. There are other options for Chinese NLP tasks, including modules from Stanford, Fudan etc. I would welcome feedback on whether adopting different Chinese NLP models/modules would significantly affect the outcomes here.\n",
    "\n",
    "## 1.1 DATA PRE-PROCESSING\n",
    "Download the original CSV files from [Twitter](https://blog.twitter.com/en_us/topics/company/2019/information_operations_directed_at_Hong_Kong.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (15,19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Reminder that the raw1/2 CSV files are NOT in this repo. Download directly from Twitter; link above\n",
    "raw1 = pd.read_csv('../data/china_082019_1_tweets_csv_hashed.csv')\n",
    "raw2 = pd.read_csv('../data/china_082019_2_tweets_csv_hashed.csv')\n",
    "raw = pd.concat([raw1, raw2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "raw = raw.drop(\n",
    "    columns=[\n",
    "        \"user_profile_url\",\n",
    "        \"tweet_client_name\",\n",
    "        \"in_reply_to_tweetid\",\n",
    "        \"in_reply_to_userid\",\n",
    "        \"quoted_tweet_tweetid\",\n",
    "        \"is_retweet\",\n",
    "        \"retweet_userid\",\n",
    "        \"retweet_tweetid\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"quote_count\",\n",
    "        \"reply_count\",\n",
    "        \"like_count\",\n",
    "        \"retweet_count\",\n",
    "        \"urls\",\n",
    "        \"user_mentions\",\n",
    "        \"poll_choices\",\n",
    "        \"hashtags\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting timings to HK time, and extracting year-month-day-hour cols\n",
    "raw['tweet_time'] = pd.to_datetime(raw['tweet_time'])\n",
    "raw['tweet_time'] = raw['tweet_time'].dt.tz_localize('GMT').dt.tz_convert('Hongkong')\n",
    "raw['tweet_year'] = raw['tweet_time'].dt.year\n",
    "raw['tweet_month'] = raw['tweet_time'].dt.month\n",
    "raw['tweet_day'] = raw['tweet_time'].dt.day\n",
    "raw['tweet_hour'] = raw['tweet_time'].dt.hour\n",
    "\n",
    "raw['account_creation_date'] = pd.to_datetime(raw['account_creation_date'], yearfirst=True)\n",
    "raw['year_of_account_creation'] = raw['account_creation_date'].dt.year\n",
    "raw['month_of_account_creation'] = raw['account_creation_date'].dt.month\n",
    "raw['day_of_account_creation'] = raw['account_creation_date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DATA FILTERING + CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll focus only on tweets sent from 2017\n",
    "raw = raw[(raw[\"tweet_year\"] >= 2017)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out tweets which mention fugitive Chinese billionaire Guo Wengui, \n",
    "# and other irrelevant characters like US-based dissidents Yang Jianli, Guo Baosheng etc\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"郭文贵\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"文贵\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"郭文\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"杨建利\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"郭宝胜\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"宝胜\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"老郭\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"郭狗\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"郭骗子\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"余文生\")].copy()\n",
    "raw = raw[~raw[\"tweet_text\"].str.contains(\"吴小晖\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, I'll focus only on Chinese tweets. \n",
    "# In earlier drafts, I found that troll accounts with English language settings were sending out Chinese tweets too,\n",
    "# so provisions were made here to include those \n",
    "# Note the sub-categories for Twitter language settings for English and Chinese\n",
    "\n",
    "raw_ch = raw[\n",
    "    (raw[\"tweet_language\"] == \"zh\")\n",
    "    & (\n",
    "        (raw[\"account_language\"] == \"en\")\n",
    "        | (raw[\"account_language\"] == \"en-gb\")\n",
    "        | (raw[\"account_language\"] == \"zh-cn\")\n",
    "        | (raw[\"account_language\"] == \"zh-CN\")\n",
    "        | (raw[\"account_language\"] == \"zh-tw\")\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121962, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ch.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into two sub-sets, one for retweets and one for \"original\" tweets\n",
    "raw_ch_no_rt = raw_ch[~raw_ch[\"tweet_text\"].str.startswith(\"RT @\")].copy()\n",
    "\n",
    "raw_ch_rt = raw_ch[raw_ch[\"tweet_text\"].str.startswith(\"RT @\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106957, 20), (15005, 20))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Surprisingly, the Chinese language tweets subset is smaller than the English one\n",
    "raw_ch_no_rt.shape, raw_ch_rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function to clean the tweet_text column and weed out non-Chinese text\n",
    "def clean_tweet_ch(text):\n",
    "    text = text.strip(\" \")\n",
    "    text = text.strip(r\"\\n\")\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    filtered = re.compile(u'[^\\u4E00-\\u9FA5]') # non-Chinese unicode range\n",
    "    text = filtered.sub(r'', text) # remove all non-Chinese characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ch_no_rt['clean_tweet_text'] = raw_ch_no_rt['tweet_text'].map(lambda tweet: clean_tweet_ch(tweet))\n",
    "\n",
    "raw_ch_rt['clean_tweet_text'] = raw_ch_rt['tweet_text'].map(lambda tweet: clean_tweet_ch(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for a sample of the cleaned up \"original\" Chinese tweets\n",
    "#raw_ch_no_rt['clean_tweet_text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for a sample of the cleaned up Chinese retweets\n",
    "# raw_ch_rt['clean_tweet_text'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DIGGING THROUGH LAYER#1 - SPAM TWEETS GALORE BUT A HINT OF CONTENT TARGETTING THE PROTEST MOVEMENT\n",
    "\n",
    "Before plotting out the frequency distribution of key words, I settled on a brief list of Chinese stop words. They are not exhaustive by any means, and reflect words which I wanted to weed out following initial drafts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb_stopwords = [\n",
    "    \"的\",\n",
    "    \"了\",\n",
    "    \"和\",\n",
    "    \"是\",\n",
    "    \"就\",\n",
    "    \"都\",\n",
    "    \"而\",\n",
    "    \"及\",\n",
    "    \"與\",\n",
    "    \"著\",\n",
    "    \"或\",\n",
    "    \"一個\",\n",
    "    \"沒有\",\n",
    "    \"我們\",\n",
    "    \"你們\",\n",
    "    \"妳們\",\n",
    "    \"他們\",\n",
    "    \"她們\",\n",
    "    \"是否\",\n",
    "    \"时间\",\n",
    "    \"整点\",\n",
    "    \"报时\",\n",
    "    \"现在\",\n",
    "    \"日电\",\n",
    "    \"月\",\n",
    "    \"日\",\n",
    "    \"桂\",\n",
    "    \"海\",\n",
    "    \"在\",\n",
    "    \"电\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /anaconda3/lib/python3.6/site-packages/jieba/dict.txt ...\n",
      "Loading model from cache /var/folders/6z/wrz4dxdx65585cc04rbtr1xh0000gn/T/jieba.cache\n",
      "Loading model cost 1.025320053100586 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "text = raw_ch_no_rt[\"clean_tweet_text\"].values\n",
    "text_list = []\n",
    "for t in text:\n",
    "    text_list.append(' '.join(jieba.cut(t, HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "一个 人           1597\n",
       "谷 歌             918\n",
       "微 信             704\n",
       "不 知道            592\n",
       "虎 扑             578\n",
       "安 卓             570\n",
       "之 家             562\n",
       "扑 讯             528\n",
       "虎 扑 讯           526\n",
       "外 媒             522\n",
       "让 你             516\n",
       "每个 人            512\n",
       "喜剧 人            503\n",
       "更 多             497\n",
       "世界 上            470\n",
       "将 于             467\n",
       "一生 平安           462\n",
       "好人 一生 平安        462\n",
       "好人 一生           462\n",
       "平安 好人           460\n",
       "你 会             458\n",
       "也 会             447\n",
       "让 人             443\n",
       "如果 你            441\n",
       "会 有             439\n",
       "你 若             431\n",
       "欢乐 喜剧 人         424\n",
       "欢乐 喜剧           424\n",
       "太 多             412\n",
       "骁 龙             398\n",
       "微 博             388\n",
       "晒 日光浴           386\n",
       "日光浴 舒服          385\n",
       "晒 日光浴 舒服        385\n",
       "好人 一生 平安 好人     385\n",
       "平安 好人 一生        385\n",
       "一生 平安 好人 一生     385\n",
       "一生 平安 好人        385\n",
       "平安 好人 一生 平安     385\n",
       "也 不             374\n",
       "给 你             363\n",
       "当 你             358\n",
       "魅 族             357\n",
       "我 不             354\n",
       "说 我             352\n",
       "你 不             348\n",
       "晒 日光浴 舒服 晒      340\n",
       "舒服 晒 日光浴        340\n",
       "舒服 晒            340\n",
       "日光浴 舒服 晒        340\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVect for subset with \"original\" tweets\n",
    "# I raised the ngram_range to (2,4) and min_df to 10 after several rounds of trial and error. \n",
    "# Dial both down if you wish to see the count for single words\n",
    "\n",
    "vect = CountVectorizer(\n",
    "    tokenizer=lambda x: x.split(), stop_words=jb_stopwords, min_df=10, ngram_range=(2, 4)\n",
    ")\n",
    "docs_ch = vect.fit_transform(text_list)\n",
    "features_ch = vect.get_feature_names()\n",
    "\n",
    "count_list1 = pd.DataFrame(docs_ch.toarray(),\n",
    "                       columns=features_ch).sum(axis=0)\n",
    "count_list1.sort_values(ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tweets_ch01.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_ch_no_rt[raw_ch_no_rt[\"clean_tweet_text\"].str.contains('一个人')].values\n",
    "# raw_ch_no_rt[raw_ch_no_rt[\"clean_tweet_text\"].str.contains('谷歌')].values\n",
    "# raw_ch_no_rt[raw_ch_no_rt[\"clean_tweet_text\"].str.contains('日光浴')].values\n",
    "# raw_ch_no_rt[raw_ch_no_rt[\"clean_tweet_text\"].str.contains('平安')].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUICK TAKE: \n",
    "Like the English tweets, the lightly filtered Chinese tweets subset is extremely noisy as the chart above shows. The most frequently used words in \"original\" tweets did not throw up phrases targetted at the HK protest movement. The tweets are instead replete with spam content involving crime stories, US-China relations, and sunbathing, of all things.....Uncomment the cells above for a flavour of the spam content in the tweets.\n",
    "\n",
    "The retweets subset below, however, threw up early hints of the content aimed directly at the protest movement, with words like \"反送中\" (anti-extradition bill), \"香港警察\" (Hong Kong police) and \"遊行\" (street rallies/protests) appearing in the top 50 list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "严重 暴力犯罪          246\n",
       "严重 暴力犯罪 案件       206\n",
       "暴力犯罪 案件          206\n",
       "依法 侦办            204\n",
       "公安机关 依法          198\n",
       "公安机关 依法 侦办       196\n",
       "中国 人             178\n",
       "一起 严重 暴力犯罪       165\n",
       "一起 严重            165\n",
       "一起 严重 暴力犯罪 案件    165\n",
       "中 美              161\n",
       "送 中              157\n",
       "侦办 一起            151\n",
       "侦办 一起 严重         151\n",
       "侦办 一起 严重 暴力犯罪    151\n",
       "中 國              149\n",
       "依法 侦办 一起         148\n",
       "依法 侦办 一起 严重      148\n",
       "政治 庇护            145\n",
       "公安机关 依法 侦办 一起    140\n",
       "香港 警察            140\n",
       "反 送 中            140\n",
       "反 送              140\n",
       "川 普              136\n",
       "反 對              131\n",
       "破 壞              121\n",
       "遊 行              120\n",
       "立法 會             120\n",
       "不 知道             119\n",
       "采取 刑事            118\n",
       "瘟 鬼              117\n",
       "刑事 强制措施          114\n",
       "采取 刑事 强制措施       114\n",
       "菜谱 作者            113\n",
       "社 會              110\n",
       "撐 警              109\n",
       "犯罪 嫌疑人           108\n",
       "逃犯 條             106\n",
       "條 例              106\n",
       "名 犯罪 嫌疑人         105\n",
       "名 犯罪             105\n",
       "逃犯 條 例           104\n",
       "阿 贵              102\n",
       "推 友              101\n",
       "无 语              100\n",
       "你 还               99\n",
       "还 能               98\n",
       "被 依法              98\n",
       "美 國               95\n",
       "也 不               94\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVect for subset with only retweets\n",
    "text = raw_ch_rt[\"clean_tweet_text\"].values\n",
    "text_list = []\n",
    "for t in text:\n",
    "    text_list.append(' '.join(jieba.cut(t, HMM=False)))\n",
    "    \n",
    "vect = CountVectorizer(\n",
    "    tokenizer=lambda x: x.split(), stop_words=jb_stopwords, min_df=10, ngram_range=(2, 4)\n",
    ")\n",
    "docs_ch = vect.fit_transform(text_list)\n",
    "features_ch = vect.get_feature_names()\n",
    "\n",
    "count_list2 = pd.DataFrame(docs_ch.toarray(),\n",
    "                       columns=features_ch).sum(axis=0)\n",
    "count_list2.sort_values(ascending = False).head(50)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below for a flavour of the spam content in the retweet subset\n",
    "#raw_ch_rt[raw_ch_rt[\"clean_tweet_text\"].str.contains('中美')].values\n",
    "#raw_ch_rt[raw_ch_rt[\"tweet_text\"].str.contains('中国')].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"retweets_ch01.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DIGGING THROUGH LAYER#2 - TWEETS ON CHINESE DISSIDENTS/FUGITIVES, COMEDIES, AND US-CHINA RELATIONS\n",
    "\n",
    "Like in Part 1, I opted to filter directly for terms which would be more relevant to the analysis, given the amount of noise in the dataset.\n",
    "\n",
    "Following earlier trials, which I won't include here, I opted for the terms 香港(Hong Kong), 政府(government), 中国(China), 顏色革命(Colour Revolution), 外國勢力(Foreign Forces), and 警察 (police)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 SIFTING THROUGH \"ORIGINAL\" CHINESE TROLL TWEETS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_no_rt1 = raw_ch_no_rt[raw_ch_no_rt['tweet_text'].str.contains(\"香港\")].copy()\n",
    "ch_no_rt2 = raw_ch_no_rt[raw_ch_no_rt['tweet_text'].str.contains(\"政府\")].copy()\n",
    "ch_no_rt3 = raw_ch_no_rt[raw_ch_no_rt['tweet_text'].str.contains(\"中国\")].copy()\n",
    "ch_no_rt4 = raw_ch_no_rt[raw_ch_no_rt['tweet_text'].str.contains(\"顏色革命\")].copy()\n",
    "ch_no_rt5 = raw_ch_no_rt[raw_ch_no_rt['tweet_text'].str.contains(\"外國勢力\")].copy()\n",
    "ch_no_rt6 = raw_ch_no_rt[raw_ch_no_rt['tweet_text'].str.contains(\"警察\")].copy()\n",
    "\n",
    "hk_ch_no_rt = pd.concat([ch_no_rt1, ch_no_rt2, ch_no_rt3, ch_no_rt4, ch_no_rt5, ch_no_rt6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 412 unique users in this subset consisting of \"original\" tweets\n",
    "hk_ch_no_rt['user_screen_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ueV2dN3HOL24gOeDeNDZzDn0LU+68V9kby+jDR2pm4=     1085\n",
       "ingaibragimova1                                  834\n",
       "HKpoliticalnew                                   648\n",
       "charikamci                                       551\n",
       "mauricerowleyx                                   469\n",
       "zGKXuesfHo+nPb6rrSG61fpwFuGLKslqTK6weUoKWTI=     392\n",
       "KondratevFortu                                   160\n",
       "ctcc507                                          159\n",
       "bagaudinzhigj                                    126\n",
       "gdvcgsfsg                                        124\n",
       "Name: user_screen_name, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see some familiar troll accounts pop up again, particularly HKpoliticalnew and ctcc507\n",
    "# Both accounts were active in the English set in Part 1 as well\n",
    "hk_ch_no_rt['user_screen_name'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@SuiBianXiaoGe 或许一觉醒来，或许明天，或许不久的将来，香港人就会明白不论是运行《逃犯条例》还是暂停修订《逃犯条例》都是中国对香港人民的爱和包容，都是为了维护香港治安稳定和香港人“民主”的良苦用心。而西方反华势力试图在香港推动的颜色革命最终会被中国人民齐心协力扼杀在摇篮中。',\n",
       "       '港版微信支付升级！“无现金”生活还远吗？ #微信 支付近日宣布，在香港推出一系列升级支付功能，标志着香港在“无现金”支付的路上又迈进了一步。 香港消费者通过微信扫描二维码可完成线上线下、不同消费场景的支付，而香港的士、报刊亭、超… https://t.co/mblIgvjTIK https://t.co/JC4uxCwltT',\n",
       "       '11月2日下午消息，双11前夕，支付宝在海外在线支付陆续落地。许多国家和地区的支付机构提前几个月，就开始联合支付宝升级自己的在线支付网络，菲律宾和香港的本地钱包今年还将首次参与“双11”。',\n",
       "       '認識傳染性極強的麻疹：紐約、泰國、香港各地紛紛爆發，有哪些症狀該如何預防？ https://t.co/Tg9UhwAVa8',\n",
       "       '10月30日， #香港科技大学 宣布，该校机器人研究所的师生研发了香港首部拥有多项创新功能的 #无人车。 此外，他们还研发了一个特别设计的控制台，能统一控制无人车的动态功能，包括电线驱动的转向、加速及制动功能。 https://t.co/E0R2ryskkl https://t.co/WzK4rcR2Ee'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This troll account was very active in the English subset as well\n",
    "hk_ch_no_rt[hk_ch_no_rt['user_screen_name']=='ctcc507']['tweet_text'].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['香港将修例禁止电子烟：违者最高罚5万港元及监禁半年 https://t.co/NlL2VYV28X',\n",
       "       '特斯拉在香港建立充电站：有50个充电位 为亚洲最大 https://t.co/fZCvguZaSy',\n",
       "       '中国联通宣布与香港、澳门电讯合作：精品网延迟1毫秒 https://t.co/NKahtMVhUM',\n",
       "       '消息称香港将向6家公司发放数字银行牌照 腾讯小米等在内 https://t.co/AlxLAnTYM0',\n",
       "       '香港中文大学开设首个人工智能学位课程 https://t.co/xeloTj0bGX'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The key words filter don't always accurately filter for content targetting the protest movement\n",
    "# This user's tweets seemed mostly irrelevant\n",
    "hk_ch_no_rt[hk_ch_no_rt['user_screen_name']=='ingaibragimova1']['tweet_text'].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['千名黑衣人圍堵立會，當中不乏重裝上陣的人，警方一定要嚴防，大家小心！👍💪\\n\\n#香港 #大專學界 #升級行動 #包圍立會 #重裝上陣 #警察 \\n原圖：星島日報 https://t.co/DIoiFWWkBo',\n",
       "       '立會經暴亂一役，幾近淪為廢墟。\\n\\n據《星島日報》報道，昨日，警方組織罪案及三合會調查科到立會蒐證調查，已掌握十名暴徒資料，他們事涉暴力衝擊包圍等事，並於日內拉人。本報於早上亦曾報道，執法者或會拘捕起警底作欺凌者。\\n\\n#香港 #立法會大樓 #星島日報 #暴徒 #日內拉人 https://t.co/m0KCaYLAkx',\n",
       "       '作為教育工作者理應「傳道、授業、解惑」，對於修訂《逃犯條例》這個關係香港法治的重大議題，更應該對學生負起「講清楚、講明白」的責任。\\nhttps://t.co/XgbK2aSG6s港人博評/45389/反修例誤導年輕人走上暴力衝擊邪路?fbclid=IwAR0tp45O3W7PdltdwfWaFlAvkweUfMEiImn8nyrF61vQVvLQwu3p6PoPZk8#selected',\n",
       "       '#西班牙 嘅警察對付巴斯克分離份子。棍棍打下去，絕唔手軟，完全合法，絕無手尾。\\n對比之下，系唔系覺得 #香港警察 確實已經很溫柔了？ https://t.co/rhHIXiPUn8',\n",
       "       '#香港 反對派一再散布失實資訊抹黑修例、誤導市民，更不斷造謠煽動情緒，企圖以網民壓力杯葛支持修例甚至只是沉默的商戶。\\n政府決定停止修例工作後，反對派又繼續播謠，稱換領智能身份證會「失去選民資格」，企圖撕裂市民對社會不同界別、團體、組織的信任。 https://t.co/9B3xCI9MWv'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This troll account was very active in the English subset as well\n",
    "hk_ch_no_rt[hk_ch_no_rt['user_screen_name']=='HKpoliticalnew']['tweet_text'].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "條 例       229\n",
       "特 區       218\n",
       "中国 人      212\n",
       "港 獨       197\n",
       "反 對       194\n",
       "立法 會      189\n",
       "香港 警察     183\n",
       "美 國       180\n",
       "區 政府      177\n",
       "特 區 政府    172\n",
       "逃犯 條 例    170\n",
       "逃犯 條      170\n",
       "中国 驻      164\n",
       "社 會       162\n",
       "中 美       158\n",
       "对 中国      147\n",
       "反 對 派     140\n",
       "對 派       140\n",
       "中 國       138\n",
       "民族 黨      135\n",
       "林 鄭       129\n",
       "香港 特      125\n",
       "總 部       120\n",
       "修 例       119\n",
       "行 動       117\n",
       "香港 特 區    117\n",
       "专项 行动     113\n",
       "勢 力       112\n",
       "學 生       112\n",
       "發 展       110\n",
       "會 議       109\n",
       "与 中国      108\n",
       "香港 人      108\n",
       "中国 法律     107\n",
       "記 者       106\n",
       "外 國       106\n",
       "修 訂       105\n",
       "逃犯 条例     103\n",
       "議 員       100\n",
       "撐 警        99\n",
       "中国 游客      98\n",
       "國 家        97\n",
       "大 學        97\n",
       "两 国        94\n",
       "鄭 娥        93\n",
       "林 鄭 娥      93\n",
       "香港 民族      93\n",
       "一 個        91\n",
       "內 地        90\n",
       "外 媒        90\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sticking to the same min_df and ngram_range values\n",
    "\n",
    "text = hk_ch_no_rt[\"clean_tweet_text\"].values\n",
    "text_list = []\n",
    "for t in text:\n",
    "    text_list.append(' '.join(jieba.cut(t, HMM=False)))\n",
    "    \n",
    "vect = CountVectorizer(\n",
    "    tokenizer=lambda x: x.split(), stop_words=jb_stopwords, min_df=10, ngram_range=(2, 4)\n",
    ")\n",
    "\n",
    "docs_ch = vect.fit_transform(text_list)\n",
    "features_ch = vect.get_feature_names()\n",
    "\n",
    "count_list3 = pd.DataFrame(docs_ch.toarray(),\n",
    "                       columns=features_ch).sum(axis=0)\n",
    "count_list3.sort_values(ascending = False).head(50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tweets_ch02.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hk_ch_no_rt[hk_ch_no_rt[\"tweet_text\"].str.contains('反對派')].values\n",
    "# hk_ch_no_rt[hk_ch_no_rt[\"tweet_text\"].str.contains('香港警察')].values\n",
    "# hk_ch_no_rt[hk_ch_no_rt[\"tweet_text\"].str.contains('港獨')].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUICK TAKE: \n",
    "\n",
    "The most active trolls, as we can see here and in Part 1, are posting content in English and Chinese, the two primary languages at play here. HKpoliticalnew and ctcc507, for instances, are tweeting in both languages.\n",
    "\n",
    "The direct filtering also gives us a better sense of the messages that the trolls were trying to push. A sample:\n",
    "\n",
    "- @charikamci, tweeting at 2018-08-19 00:26:00+08:00: '香港回歸至今反對派不時興風作浪年爆發日違法佔中反對派倒打一耙將責任推到特區政府和中央身上大公報獲得的最新機密文件顯示佔中其實是亂港派與境外反華勢力精心策動的一場顏色革命旨在利用年輕人妄圖一夜變天']\n",
    "\n",
    "- @Resilceale; tweeting at 2019-07-09 12:17:00+08:00: 以「反修例」為借口，以學生為棋子，以打砸為手段，以分化為目的，香港反對派不惜重金招募，幕後指使學生 「沖擊立法會」，讓「東方明珠」嘅天空布滿陰霾，港民應擦亮慧眼！\"\n",
    "\n",
    "- @qujianming1; tweeting at 2019-07-02 10:14:00+08:00: '香港七一游行民阵月日反对派宣扬暴力冲击立法会的举动是对香港百年法治精神的践踏将和平游行演变成颜色革命令人愤慨此时刻香港市民应该保持克制冷静用理性和平的方式表达意见携手香港警察共同维护社会稳定社会各界更应冷静理性共同促进香港法治进步'\n",
    "\n",
    "- @maksmkas6g; tweeting at 2019-07-04 17:11:00+08:00: '@bindarsou 打掉这帮暴徒的嚣张，香港警察加油'\n",
    "\n",
    "- @Resilceale, tweeting at 2019-07-09 12:15:00+08:00: '香港嘅「港獨」分子，為了達到不可告人之目的，煽動不明真相嘅青年人參與游行，甚至進行所謂嘅打砸搶違法行為向政府施壓。仲系醒醒吧，少干點蠢事，多長點心眼！世界上冇邊個國家允許呢種猖狂行徑盛行嘅，唔系唔打，肯定要打掉嘅！而且要重打！\n",
    "\n",
    "Uncomment the lines above to see a fuller set of the tweets. The content in itself is not surprising - essentially backing the Hong Kong police and blaming secessionist groups and \"hostile foreign forces\" for trying to forment a colour revolution.\n",
    "\n",
    "A detailed content analysis, however, is outside the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 SIFTING THROUGH CHINESE TROLL RETWEETS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_rt1 = raw_ch_rt[raw_ch_rt['tweet_text'].str.contains(\"香港\")].copy()\n",
    "ch_rt2 = raw_ch_rt[raw_ch_rt['tweet_text'].str.contains(\"政府\")].copy()\n",
    "ch_rt3 = raw_ch_rt[raw_ch_rt['tweet_text'].str.contains(\"中国\")].copy()\n",
    "ch_rt4 = raw_ch_rt[raw_ch_rt['tweet_text'].str.contains(\"顏色革命\")].copy()\n",
    "ch_rt5 = raw_ch_rt[raw_ch_rt['tweet_text'].str.contains(\"外國勢力\")].copy()\n",
    "ch_rt6 = raw_ch_rt[raw_ch_rt['tweet_text'].str.contains(\"警察\")].copy()\n",
    "\n",
    "hk_ch_rt = pd.concat([ch_rt1, ch_rt2, ch_rt3, ch_rt4, ch_rt5, ch_rt6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_ch_rt['user_screen_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FWE41OnopBB5h3unPH3s3XBA3t3zEuROlhnEue2H8cE=    369\n",
       "benjaminkudla39                                 110\n",
       "HKpoliticalnew                                   62\n",
       "GirgRKG36vs7eBx81goMAn6AlVfUp0RjKLwCCdd7aU=      60\n",
       "ardansuweb                                       60\n",
       "randxr89c                                        44\n",
       "w9pbfQRUXBcO810z7Q9I5TbWnGdbZaBB3Gvh6KxT6Y=      39\n",
       "fMPA8G2z8yjkrEMdZkNSnRraECis1zX6tv2N7NcF7aY=     34\n",
       "2l1eDka0eiClBUYoDXlwYaKcUaeelnz44aDM9OJRM=       31\n",
       "Melissa64269411                                  28\n",
       "Name: user_screen_name, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_ch_rt['user_screen_name'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RT @simsoer: 曾經亞洲最安全城市嘅締造者-香港警察，為了最小嘅傷害保持了最大嘅克制。 嚴正聲明——強力支持香港警察嘅執法行為，強烈譴責香港反修例暴徒暴力嘅行為，愿香港重歸穩定，齊心建設粵港澳灣區 #HK #HongKongProtest \\nhttps://t.co/…',\n",
       "       'RT @CNS1952: 佳士得香港2019年春季拍卖会将于24日在香港会展中心开槌。为期一周的春拍将呈献中国书画、中国瓷器及艺术品、亚洲二十世纪及当代艺术、珠宝名表等多个专场，拍品横跨古代至当代的东西方艺术。 https://t.co/BtQYX2Egyr',\n",
       "       'RT @CNS1952: 7月8日，由香港贸易发展局主办的第26届香港时装节春夏系列假湾仔香港会议展览中心举行。一连四日的展览以Oriental Fever为布展主题，吸引来自12个国家及地区约一千家参展商参与。来看看开幕时装巡礼表演吧！ https://t.co/Et045p…',\n",
       "       'RT @CNS1952: 5月22日，中国一支民间女子登山队成功登顶珠穆朗玛峰。分别来自新疆、香港、河南的马丽娅姆、曾燕红、孙宁相继登顶。从4月8日出发至今，她们历经40多天到达世界之巅。目前，女子登山队的全部成员正在安全下撤途中。#中国故事 https://t.co/q9JT…',\n",
       "       'RT @CNS1952: 香港特别行政区一些极端激进分子1日以极为暴力的方式冲击立法会大楼，肆意损坏立法会设施，海外专家学者对此纷纷发声谴责，认为这是践踏法治、危害社会秩序的严重违法行为，并呼吁外国势力停止插手香港事务和中国内政。https://t.co/NDVO6CxZZl…'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There's still noise in these filtered tweets, evidenced by the tweets on a fashion festival and art auction\n",
    "hk_ch_rt[hk_ch_rt['user_screen_name']=='ardansuweb']['tweet_text'].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RT @aptib08e7: #香港 #hongkong 睇到暴力行為和活動，首先我認為作為一個香港人，就應該從自己做起，從一個維護香港社會大局穩定嘅角度出發，自覺抵觸呢種暴力嘅活動，並大聲嘅呼吁社會各界盡快冷靜落嚟，唔好參與諸如此類嘅活動，香港需要嘅系和平、發展、繁榮，而唔系…',\n",
       "       'RT @simsoer: 曾經亞洲最安全城市嘅締造者-香港警察，為了最小嘅傷害保持了最大嘅克制。 嚴正聲明——強力支持香港警察嘅執法行為，強烈譴責香港反修例暴徒暴力嘅行為，愿香港重歸穩定，齊心建設粵港澳灣區 #HK #HongKongProtest \\nhttps://t.co/…',\n",
       "       'RT @anafedinzp: #HongKong #HK 眾所周知，香港警隊作為維護香港安定嘅守護者往往承擔著比其他職業更多嘅社會責任和壓力。面對近期呢一系列衝突事件，香港警方面對已成騷亂嘅暴力行徑被迫採取咗必要嘅處置手段，既保持咗剋制，態度又堅決。我哋認為，香港警方表現出嘅…',\n",
       "       'RT @CNS1952: 原定28日进行拍卖的清代画家任伯年画作《澹黄杨柳带栖鸭》于26日预展时被一名小童撕毁。佳士得香港方面称已通知委托方，并撤拍该作品。被毁的画作为创作于1889年的《花鸟四屏》中的一幅。整组作品估价为150万至250万港元（约合19至31万美元）。http…',\n",
       "       'RT @CNS1952: 2019香港玩具节16日在香港会展中心继续举行。本届玩具节吸引众多玩具迷蜂拥入场，除了儿童以外，亦有众多成年玩具迷前来参观、选购潮流玩具和怀旧玩具。 https://t.co/qnUBRQPX58'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_ch_rt[hk_ch_rt['user_screen_name']=='randxr89c']['tweet_text'].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "香港 警察       285\n",
       "送 中         207\n",
       "撐 警         205\n",
       "反 對         199\n",
       "中国 人        189\n",
       "反 送         181\n",
       "反 送 中       181\n",
       "行 動         159\n",
       "社 會         156\n",
       "逃犯 條        137\n",
       "條 例         137\n",
       "逃犯 條 例      135\n",
       "破 壞         131\n",
       "行 為         127\n",
       "遊 行         126\n",
       "我 哋         126\n",
       "香港 撐        117\n",
       "對 派         117\n",
       "反 對 派       117\n",
       "立法 會        115\n",
       "中 香港        115\n",
       "送 中 香港      115\n",
       "執 法         112\n",
       "修 例         110\n",
       "暴 動         107\n",
       "警察 嘅        106\n",
       "學 生         102\n",
       "嚴 正         101\n",
       "穩 定          96\n",
       "警 隊          95\n",
       "譴 責          92\n",
       "反 送 中 香港     89\n",
       "中 美          86\n",
       "勢 力          83\n",
       "警 行          83\n",
       "警 行 動        83\n",
       "撐 警 行 動      83\n",
       "撐 警 行        83\n",
       "反修 例         82\n",
       "撐 警 隊        82\n",
       "香港 撐 警 行     81\n",
       "你 我          81\n",
       "香港 撐 警       81\n",
       "隊 你 我 同行     80\n",
       "撐 警 隊 你      80\n",
       "警 隊 你        80\n",
       "警 隊 你 我      80\n",
       "隊 你          80\n",
       "你 我 同行       80\n",
       "我 同行         80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = hk_ch_rt[\"clean_tweet_text\"].values\n",
    "text_list = []\n",
    "for t in text:\n",
    "    text_list.append(' '.join(jieba.cut(t, HMM=False)))\n",
    "    \n",
    "vect = CountVectorizer(\n",
    "    tokenizer=lambda x: x.split(), stop_words=jb_stopwords, min_df=10, ngram_range=(2, 4)\n",
    ")\n",
    "\n",
    "docs_ch = vect.fit_transform(text_list)\n",
    "features_ch = vect.get_feature_names()\n",
    "\n",
    "count_list4 = pd.DataFrame(docs_ch.toarray(),\n",
    "                       columns=features_ch).sum(axis=0)\n",
    "count_list4.sort_values(ascending = False).head(50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"retweets_ch02.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hk_ch_rt[hk_ch_rt[\"tweet_text\"].str.contains('反對派')].values\n",
    "# hk_ch_rt[hk_ch_rt[\"tweet_text\"].str.contains('香港警察')].values\n",
    "# hk_ch_rt[hk_ch_rt[\"tweet_text\"].str.contains('港獨')].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retweets push a similar narrative - praising the police for taking action and condemning the violent protests. Uncomment the cell above for a fuller look. A sample: \n",
    "\n",
    "'RT @bindarsou: 香港警察真正嘅正義使者反對派污蔑攻击香港警察嘅目的係為咗擾亂香港嘅社會穩定相信廣大具有正義感嘅市民同學生定能睇清事實不再被反對派制造嘅假象迷惑譴責利用學生進行暴力嘅邋遢行為香港警察撐港警反暴力遊行'\n",
    "\n",
    "'RT @simsoer: 曾經亞洲最安全城市嘅締造者香港警察為了最小嘅傷害保持了最大嘅克制嚴正聲明強力支持香港警察嘅執法行為強烈譴責香港反修例暴徒暴力嘅行為愿香港重歸穩定齊心建設粵港澳灣區'\n",
    "\n",
    "'RT @GolloglyAlysha: #香港 #逃犯條例 看看美國是怎麼培養港獨勢力的這些所謂的青年精英無非是美國人的棋子禍港亂港的真兇'\n",
    "\n",
    "Support for the HK police (with the phrase/hashtag \"撐警\", which has come under intense criticisms for its handling of the protests, appear to be more prominent in the rewteets subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DIGGING THROUGH LAYER#3 - THE TOP TROLLS\n",
    "\n",
    "Like in Part 1, I'll sift through the accounts most active in tweeting or retweeting content targetting the HK protest movement. The 10 accounts I picked below are not exhaustive by any means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 SIFTING THROUGH TOP TROLL RETWEETS IN CHINESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trolls_ch = [\n",
    "    \"HKpoliticalnew\",\n",
    "    \"charikamci\",\n",
    "    \"ctcc507\",\n",
    "    \"KondratevFortu\",\n",
    "    \"jdhdnchsdh\",\n",
    "    \"shaunta58sh\",\n",
    "    \"Resilceale\",\n",
    "    \"qujianming1\",\n",
    "    \"vezerullasav158\",\n",
    "    \"ardansuweb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1817, 21)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with \"original\" tweets, in Chinese, by the top trolls\n",
    "top_ch_trolls = hk_ch_no_rt[hk_ch_no_rt['user_screen_name'].isin(trolls_ch)]\n",
    "top_ch_trolls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "特 區          191\n",
       "港 獨          189\n",
       "條 例          189\n",
       "立法 會         181\n",
       "美 國          171\n",
       "反 對          160\n",
       "區 政府         155\n",
       "特 區 政府       152\n",
       "逃犯 條 例       140\n",
       "逃犯 條         140\n",
       "民族 黨         133\n",
       "林 鄭          129\n",
       "香港 特         114\n",
       "對 派          113\n",
       "反 對 派        113\n",
       "行 動          112\n",
       "社 會          111\n",
       "總 部          111\n",
       "香港 特 區       109\n",
       "學 生          107\n",
       "外 國          106\n",
       "記 者          106\n",
       "會 議          104\n",
       "勢 力          102\n",
       "中 國          102\n",
       "撐 警           97\n",
       "發 展           97\n",
       "修 訂           95\n",
       "議 員           95\n",
       "鄭 娥           93\n",
       "林 鄭 娥         93\n",
       "香港 民族         91\n",
       "大 學           91\n",
       "陳 浩           89\n",
       "內 地           89\n",
       "國 家           86\n",
       "佔 中           86\n",
       "香港 民族 黨       86\n",
       "修 例           83\n",
       "陳 浩 天         81\n",
       "浩 天           81\n",
       "色 革命          80\n",
       "長 官           80\n",
       "行政 長 官        80\n",
       "行政 長          80\n",
       "顏 色           76\n",
       "顏 色 革命        75\n",
       "警 總           74\n",
       "活 動           74\n",
       "香港 特 區 政府     72\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = top_ch_trolls[\"clean_tweet_text\"].values\n",
    "text_list = []\n",
    "for t in text:\n",
    "    text_list.append(' '.join(jieba.cut(t, HMM=False)))\n",
    "    \n",
    "vect = CountVectorizer(\n",
    "    tokenizer=lambda x: x.split(), stop_words=jb_stopwords, min_df=10, ngram_range=(2, 4)\n",
    ")\n",
    "docs_ch = vect.fit_transform(text_list)\n",
    "features_ch = vect.get_feature_names()\n",
    "\n",
    "count_list5 = pd.DataFrame(docs_ch.toarray(),\n",
    "                       columns=features_ch).sum(axis=0)\n",
    "count_list5.sort_values(ascending = False).head(50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"toptrolls_tweets.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_ch_trolls[top_ch_trolls[\"tweet_text\"].str.contains('特區')].values\n",
    "# top_ch_trolls[top_ch_trolls[\"tweet_text\"].str.contains('港獨')].values\n",
    "# top_ch_trolls[top_ch_trolls[\"tweet_text\"].str.contains('美國')].values\n",
    "# top_ch_trolls[top_ch_trolls[\"tweet_text\"].str.contains('顏色革命')].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUICK TAKE:\n",
    "The top trolls are far more aggressive in pushing the conspiracy theories about the US and other \"hostile foreign forces\" being behind the HK protests. \n",
    "\n",
    "@HKpoliticalnew tweeted on July 1 2019, for instance, that the staff at the UK Consulate in HK, some whom it accused of being \"CIA\" agents, were behind efforts to destablise Hong Kong. On June 13, the same account also sent out a tweet alleging that foreign intelligence agents were working undercover as journalists in order to disrupt the work of the police.\n",
    "\n",
    "Another user, @KondratevFortu alleged that Apple Daily founder Jimmy Lai had flown to the US to conspire with the US to \"interfere\" with Hong Kong's affairs. The tweet was sent on the same day where the [Chinese foreign ministry slammed Mr Lai and top US officials](https://www.scmp.com/news/hong-kong/politics/article/3017868/beijing-foreign-office-slams-hong-kong-tycoon-jimmy-lai-and) for interfering in HK and China's \"internal affairs\".\n",
    "\n",
    "A sample of the tweets below. Uncomment the cell above to see the tweets in full:\n",
    "\n",
    "- @HKpoliticalnew; tweeting at 2019-06-18 16:38:00+08:00: 美國資助「港獨」廢青洗腦\\n#諜影重重 #顏色革命 #香港 https://t.co/qZvsZQpwoa'\n",
    "\n",
    "- @HKpoliticalnew; tweeting at 2019-07-01 05:12:00+08:00: '美國香港領使館千人員工，不小是CIA特工連同香港漢奸在港推動顏色革命 Over 1,000 US Consulate staffs, many with CIA launched Color Revolution to destabilize HK 香港警察「阿sir我撐你」 支持者逼爆添馬。\\n\\n多名藝人藍衣撐警 譚詠麟：家和萬事興。\\n\\n#香港 #撐警行動 https://t.co/alO0lhkMc6']\n",
    "\n",
    "\n",
    "- @HKpoliticalnew, tweeting at 2019-06-22 22:16:00+08:00: 美國喺香港推動顏色革命 總指揮嚟自香港美國領事館 下令一定要制造流血事件 美國要見血讓其媒稱...\n",
    "\n",
    "\n",
    "- @HKpoliticalnew; tweeting at 2019-06-13 23:38:00+08:00: '深扒內幕特工扮演記者傳媒其實是暗地裡擔任暴亂指揮工作有時故意扮採訪拍攝而擋在暴徒前面阻礙警察工作執勤反送中現場外國特工指揮暴亂顏色革命香港時政直擊'\n",
    "\n",
    "- @KondratevFortu, tweeting at 2019-07-09 14:38:00+08:00 '有「佔中黑手」之稱的香港壹傳媒創辦人黎智英赴美勾連外國勢力干預香港事務。美國國務院發言人奧塔格斯（Morgan Ortagus）周一（8日）發新聞稿，表示美國國務卿蓬佩奧，當日於首都華盛頓與黎智英會晤，討論有關香港修訂《逃犯條例》的發展，以及香港在「一國兩制」框架下的自治地位等問題。 https://t.co/gHSxwgP1Xa'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SIFTING THROUGH TOP TROLL RETWEETS IN CHINESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 21)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the subset for retweets, in Chinese, by the top trolls\n",
    "top_ch_trolls_rt = hk_ch_rt[hk_ch_rt['user_screen_name'].isin(trolls_ch)]\n",
    "top_ch_trolls_rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "撐 警         57\n",
       "香港 警察       47\n",
       "社 會         37\n",
       "行 動         34\n",
       "香港 撐        33\n",
       "我 哋         30\n",
       "行 為         29\n",
       "立法 會        27\n",
       "警察 嘅        26\n",
       "反 對         25\n",
       "對 派         24\n",
       "反 對 派       24\n",
       "警 行 動       24\n",
       "警 行         24\n",
       "撐 警 行       24\n",
       "撐 警 行 動     24\n",
       "香港 撐 警      23\n",
       "香港 撐 警 行    23\n",
       "嚴 正         23\n",
       "譴 責         23\n",
       "執 法         22\n",
       "哋 需要 你      20\n",
       "警 隊         20\n",
       "警 隊 你       20\n",
       "警 隊 你 我     20\n",
       "哋 需要        20\n",
       "撐 警 隊 你     20\n",
       "學 生         20\n",
       "撐 警 隊       20\n",
       "需要 你        20\n",
       "我 同行        20\n",
       "你 我 同行      20\n",
       "隊 你 我 同行    20\n",
       "隊 你 我       20\n",
       "隊 你         20\n",
       "你 我         20\n",
       "我 哋 需要      20\n",
       "我 哋 需要 你    20\n",
       "逃犯 条例       16\n",
       "穩 定         16\n",
       "暴力 嘅        16\n",
       "破 壞         16\n",
       "香港 警察 嘅     16\n",
       "正 執 法       14\n",
       "社 會 治安      14\n",
       "嚴 正 執 法     14\n",
       "會 治安        14\n",
       "連 續         14\n",
       "嚴 正 執       14\n",
       "正 執         14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = top_ch_trolls_rt[\"clean_tweet_text\"].values\n",
    "text_list = []\n",
    "for t in text:\n",
    "    text_list.append(' '.join(jieba.cut(t, HMM=False)))\n",
    "    \n",
    "vect = CountVectorizer(\n",
    "    tokenizer=lambda x: x.split(), stop_words=jb_stopwords, min_df=10, ngram_range=(2, 4)\n",
    ")\n",
    "docs_ch = vect.fit_transform(text_list)\n",
    "features_ch = vect.get_feature_names()\n",
    "\n",
    "count_list6 = pd.DataFrame(docs_ch.toarray(),\n",
    "                       columns=features_ch).sum(axis=0)\n",
    "count_list6.sort_values(ascending = False).head(50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"toptrolls_retweets.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_ch_trolls_rt[top_ch_trolls_rt[\"tweet_text\"].str.contains('嚴正')].values\n",
    "# top_ch_trolls_rt[top_ch_trolls_rt[\"tweet_text\"].str.contains('撐警行動')].values\n",
    "# top_ch_trolls_rt[top_ch_trolls_rt[\"tweet_text\"].str.contains('社會治安')].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retweets continued the earlier trend of showing support for the HK police. This came frequently via the retweets of one of the top troll accounts @HKpoliticalnew. Uncomment the cell above for fuller details. A sample of the tweets:\n",
    "\n",
    "- 'RT @HKpoliticalnew: 「我哋需要你」【1】\\n\\n連日嚟多個團體喺中環遮打花園等地舉辦撐警晚會、街站，精力更有大批人員到警總高呼「我哋需要你」，大叫「支持警察，嚴正執法，愛香港，撐警察」嘅口號。\\n#香港 #撐警行動 https://t.co/1gp6JqhumY'\n",
    "\n",
    "- 'RT @HKpoliticalnew: 暴徒們連續性嘅去街嚴重破壞了香港社會治安秩序，毆打執勤警員…\\n7月1日暴徒們更系大肆使用暴力，肆意沖擊破壞立法會大樓，而反對派議員非但未阻止示威者武力升級，更叫警員「克制」，仲為暴徒提供人力及物資，煽風點火…\\n呢種時候只能冀警方嚴正執法，…',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 4, I'll use the Scattertext tool to produce interactive visualisations of the English troll tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
